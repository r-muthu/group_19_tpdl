{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75f6ce0b-d14f-40a2-bd3a-b43f8ad2a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib pandas torch torchmetrics scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ad0de771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.classification import Accuracy\n",
    "from model import ResNet50 as model\n",
    "# import os\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a6624bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "43b94435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, npy_file_paths, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            npy_file_paths (list of str): List of file paths for .npy files containing the sequences.\n",
    "            labels (list): List of labels corresponding to each sequence.\n",
    "        \"\"\"\n",
    "        # Load the sequences and labels\n",
    "        self.data = [torch.tensor(np.load(file_path)) for file_path in npy_file_paths]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)  # Convert the labels to a tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        # Dataset contains as many samples as the number of npy files\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the sequence data and its corresponding label\n",
    "        return self.data[idx], self.labels[idx].long()\n",
    "\n",
    "# Example usage\n",
    "npy_file_paths = []\n",
    "labels = []\n",
    "\n",
    "root_dir = 'data/mel'\n",
    "languages = sorted([d for d in os.listdir(root_dir) if d != '.ipynb_checkpoints' and os.path.isdir(os.path.join(root_dir, d))])\n",
    "num_languages = len(languages)\n",
    "\n",
    "for i, lang_dir in enumerate(languages):\n",
    "    lang_path = os.path.join(root_dir, lang_dir)\n",
    "    if not os.path.isdir(lang_path):\n",
    "        continue  # Skip non-directory files\n",
    "\n",
    "    # List all .npy files\n",
    "    file_names = os.listdir(lang_path)\n",
    "    full_paths = [os.path.join(lang_path, f) for f in file_names]\n",
    "    \n",
    "    # One-hot label for this language\n",
    "    one_hot_label = np.zeros(num_languages)\n",
    "    one_hot_label[i] = 1\n",
    "\n",
    "    # Extend lists\n",
    "    npy_file_paths.extend(full_paths)\n",
    "    labels.extend([i] * len(full_paths))  # just the class index\n",
    "\n",
    "\n",
    "dataset = SequenceDataset(npy_file_paths, labels)\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9e57df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    # Split dataset into train, validation, and test sets\n",
    "    train_data, temp_data = train_test_split(dataset, test_size=0.3, shuffle=True)\n",
    "    valid_data, test_data = train_test_split(temp_data, test_size=0.5, shuffle=True)\n",
    "\n",
    "    # Define batch size for training, full batch for validation and testing\n",
    "    batch_size = 128\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)  # Mini-batch for training\n",
    "    valid_loader = DataLoader(valid_data, batch_size=len(valid_data), shuffle=False)  # Full batch for validation\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)  # Full batch for testing\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1236705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = split_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, valid_loader, test_loader, num_epochs=10, gradual_unfreezing=False, unfreeze_epochs=[]):\n",
    "        self.device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.num_epochs = num_epochs\n",
    "        self.gradual_unfreezing = gradual_unfreezing\n",
    "        self.unfreeze_epochs = (\n",
    "            [] if unfreeze_epochs is None or not gradual_unfreezing \n",
    "            else sorted(unfreeze_epochs, reverse=True)  # Unfreeze last layers first\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08)\n",
    "        self.optimizer.zero_grad()\n",
    "        self.accuracy_metric = Accuracy(task=\"multiclass\", num_classes=self.model.num_classes).to(self.device)\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model_state = None\n",
    "        self.best_epoch = 0\n",
    "        self.l2_lambda = 0.001\n",
    "        self.layer_groups = self._group_layers()\n",
    "        self._initialize_requires_grad()\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def _group_layers(self):\n",
    "        layer_dict = OrderedDict()\n",
    "        for name, param in self.model.named_parameters():\n",
    "            layer_name = name.split(\".\")[0]  # Extract top-level layer name\n",
    "            if layer_name not in layer_dict:\n",
    "                layer_dict[layer_name] = []\n",
    "            layer_dict[layer_name].append(param)\n",
    "        return list(layer_dict.items())  # List of (layer_name, parameters) tuples\n",
    "\n",
    "    def _initialize_requires_grad(self):\n",
    "        if not self.gradual_unfreezing:  # If gradual_unfreezing is False, make all parameters trainable\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for layer_name, params in self.layer_groups:\n",
    "                for param in params:\n",
    "                    param.requires_grad = False\n",
    "            self._unfreeze_layer() #unfreeze fc layer\n",
    "\n",
    "    def _unfreeze_layer_by_epoch(self, epoch):\n",
    "        if epoch in self.unfreeze_epochs:\n",
    "            self._unfreeze_layer()\n",
    "    \n",
    "    def _unfreeze_layer(self):\n",
    "        if self.layer_groups:\n",
    "            layer_name, params = self.layer_groups.pop()  # Unfreeze last remaining layer\n",
    "            for param in params:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            self._unfreeze_layer_by_epoch(epoch)\n",
    "            for inputs, targets in self.train_loader:\n",
    "                inputs, targets = inputs.float().to(self.device), targets.to(self.device)\n",
    "                # DEBUG: Check target range\n",
    "                if targets.min() < 0 or targets.max() >= self.model.num_classes:\n",
    "                    raise ValueError(\n",
    "                        f\"Found invalid label in batch: min={targets.min().item()}, max={targets.max().item()}, expected in [0, {self.model.num_classes - 1}]\"\n",
    "                    )\n",
    "                print(f\"Targets: {targets}\")\n",
    "                pred = self.model(inputs)\n",
    "                loss = self.criterion(pred, targets)\n",
    "                # L2 Regularization\n",
    "                l2_norm = sum(p.pow(2).sum() for p in self.model.parameters())\n",
    "                loss += self.l2_lambda * l2_norm\n",
    "                \n",
    "                self.accuracy_metric.update(pred, targets)\n",
    "                accuracy = self.accuracy_metric.compute()\n",
    "                self.accuracy_metric.reset()\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            val_loss, val_accuracy = self.validate()\n",
    "            self.train_losses.append(loss.item())\n",
    "            self.train_accuracies.append(accuracy.item())\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_accuracy.item())\n",
    "            \n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_model_state = self.model.state_dict()\n",
    "                self.best_epoch = epoch + 1\n",
    "                self._save_best_model()\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{self.num_epochs}], Training Loss: {loss.item():.4f}, Training Accuracy: {accuracy.item():.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        self._load_best_model()\n",
    "        self.test()\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = next(iter(self.valid_loader))\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            pred = self.model(inputs)\n",
    "            loss = self.criterion(pred, targets).item()\n",
    "            self.accuracy_metric.update(pred, targets)\n",
    "            accuracy = self.accuracy_metric.compute()\n",
    "            self.accuracy_metric.reset()\n",
    "        return loss, accuracy\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = next(iter(self.test_loader))\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            pred = self.model(inputs)\n",
    "            loss = self.criterion(pred, targets).item()\n",
    "            self.accuracy_metric.update(pred, targets)\n",
    "            accuracy = self.accuracy_metric.compute()\n",
    "            self.accuracy_metric.reset()\n",
    "        print(f'Final Test Loss: {loss:.4f}, Final Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    def _save_best_model(self):\n",
    "        with open(\"best_model.pkl\", \"wb\") as f:\n",
    "            pickle.dump({\"model_state\": self.best_model_state, \"epoch\": self.best_epoch, \"val_loss\": self.best_val_loss}, f)\n",
    "\n",
    "    def _load_best_model(self):\n",
    "        with open(\"best_model.pkl\", \"rb\") as f:\n",
    "            saved_data = pickle.load(f)\n",
    "            self.model.load_state_dict(saved_data[\"model_state\"])\n",
    "            print(f\"Best Model Achieved at Epoch: {saved_data['epoch']} with Validation Loss: {saved_data['val_loss']:.4f}\")\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        plt.plot(self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss vs. Epoch')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracies(self):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(self.train_accuracies, label='Train Accuracy')\n",
    "        plt.plot(self.val_accuracies, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy vs. Epoch')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1ba1c2f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_languages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m all_labels = torch.tensor([label \u001b[38;5;28;01mfor\u001b[39;00m _, label \u001b[38;5;129;01min\u001b[39;00m dataset])\n\u001b[32m      3\u001b[39m trainer.train()     \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, train_loader, valid_loader, test_loader, num_epochs, gradual_unfreezing, unfreeze_epochs)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, train_loader, valid_loader, test_loader, num_epochs=\u001b[32m10\u001b[39m, gradual_unfreezing=\u001b[38;5;28;01mFalse\u001b[39;00m, unfreeze_epochs=[]):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda:1\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (torch.cuda.is_available()) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_loader = train_loader\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mself\u001b[39m.valid_loader = valid_loader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/group_19_tpdl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/group_19_tpdl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/group_19_tpdl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/group_19_tpdl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/group_19_tpdl/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model(num_classes=num_languages), train_loader, valid_loader, test_loader)\n",
    "all_labels = torch.tensor([label for _, label in dataset])\n",
    "trainer.train()     \n",
    "# trainer.plot_losses()\n",
    "# trainer.plot_accuracies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603e688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
