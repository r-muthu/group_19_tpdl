{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0772201",
   "metadata": {},
   "source": [
    "### This is the trainer notebook. Run all cells sequentially. No modifications are needed unless stated\n",
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eddb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib pandas torch torchmetrics scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdecf8f6",
   "metadata": {},
   "source": [
    "### Import all libraries and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0de771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.classification import Accuracy\n",
    "from models import ResNet50, ResNet50BiLSTMAttention, ResNet34BiLSTMAttention\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "#Implemented seeding \n",
    "def seed_functions(seed):\n",
    "\t\"\"\"Seeds functions from numpy and torch.\"\"\"\n",
    "\tnp.random.seed(seed)\n",
    "\trandom.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed)\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "SEED = 37\n",
    "seed_functions(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13f548",
   "metadata": {},
   "source": [
    "### This is cell contains a custom class definition, helper function for instantiation to create a dataset of npy files (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b94435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom class defined to store dataset\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, npy_file_paths, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            npy_file_paths (list of str): List of file paths for .npy files containing the sequences.\n",
    "            labels (list): List of labels corresponding to each sequence.\n",
    "        \"\"\"\n",
    "        # Load the sequences and labels\n",
    "        self.data = [torch.tensor(np.load(file_path)) for file_path in npy_file_paths]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)  # Convert the labels to a tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        # Dataset contains as many samples as the number of npy files\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the sequence data and its corresponding label\n",
    "        return self.data[idx], self.labels[idx].long()\n",
    "\n",
    "# Helper function to create dataset\n",
    "def create_9_languages_dataset(path_to_dataset):\n",
    "    # Storing of dataset into class\n",
    "    npy_file_paths = []\n",
    "    labels = []\n",
    "\n",
    "    languages = sorted([d for d in os.listdir(path_to_dataset) if d != '.ipynb_checkpoints' and os.path.isdir(os.path.join(path_to_dataset, d))])\n",
    "    print(languages)\n",
    "\n",
    "    for i, lang_dir in enumerate(languages):\n",
    "        lang_path = os.path.join(path_to_dataset, lang_dir)\n",
    "        if not os.path.isdir(lang_path):\n",
    "            continue  # Skip non-directory files\n",
    "\n",
    "        # List all .npy files\n",
    "        file_names = os.listdir(lang_path)\n",
    "        full_paths = [os.path.join(lang_path, f) for f in file_names]\n",
    "\n",
    "        # Extend lists\n",
    "        npy_file_paths.extend(full_paths)\n",
    "        labels.extend([i] * len(full_paths))\n",
    "\n",
    "\n",
    "    dataset = SequenceDataset(npy_file_paths, labels)  # THIS IS THE FINAL DATASET\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Helper function to create dataset\n",
    "def create_4_languages_dataset(path_to_dataset):\n",
    "    # Storing of dataset into class\n",
    "    npy_file_paths = []\n",
    "    labels = []\n",
    "    four_languages = ['arabic', 'chinese', 'english', 'hindi']\n",
    "    languages = sorted([d for d in os.listdir(path_to_dataset) if d in four_languages and os.path.isdir(os.path.join(path_to_dataset, d))])\n",
    "    print(languages)\n",
    "    num_languages = len(languages)\n",
    "\n",
    "    for i, lang_dir in enumerate(languages):\n",
    "        lang_path = os.path.join(path_to_dataset, lang_dir)\n",
    "        if not os.path.isdir(lang_path):\n",
    "            continue  # Skip non-directory files\n",
    "\n",
    "        # List all .npy files\n",
    "        file_names = os.listdir(lang_path)\n",
    "        full_paths = [os.path.join(lang_path, f) for f in file_names]\n",
    "\n",
    "        # Extend lists\n",
    "        npy_file_paths.extend(full_paths)\n",
    "        labels.extend([i] * len(full_paths))\n",
    "\n",
    "\n",
    "    dataset = SequenceDataset(npy_file_paths, labels)  # THIS IS THE FINAL DATASET\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356b7ec4",
   "metadata": {},
   "source": [
    "### Replace 'None' with the path to the dataset (MODIFY HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c29218",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = 'data1' #Enter path to dataset here\n",
    "nine_language_dataset = create_9_languages_dataset(path_to_dataset)\n",
    "four_language_dataset = create_4_languages_dataset(path_to_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090d3ecc",
   "metadata": {},
   "source": [
    "### Helper function to split dataset into train, valid and test dataloaders (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    # Split dataset into train, validation, and test sets\n",
    "    indices = np.arange(len(dataset))\n",
    "    np.random.seed(SEED)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_size = int(0.7 * len(indices))\n",
    "    valid_size = (len(indices) - train_size) // 2\n",
    "    test_size = len(indices) - train_size - valid_size\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    valid_indices = indices[train_size:train_size+valid_size]\n",
    "    test_indices = indices[train_size +valid_size:]\n",
    "\n",
    "    train_data = torch.utils.data.Subset(dataset, train_indices)\n",
    "    valid_data = torch.utils.data.Subset(dataset, valid_indices)\n",
    "    test_data = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "    # Define a seed worker for DataLoader\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = SEED + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    # Define generators for DataLoader\n",
    "    generator = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "    batch_size = 128\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True,\n",
    "                              generator=generator, worker_init_fn=seed_worker, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_data, batch_size=len(valid_data), shuffle=False,\n",
    "                              generator=generator, worker_init_fn=seed_worker)\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False,\n",
    "                             generator=generator, worker_init_fn=seed_worker)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "nine_language_train_loader, nine_language_valid_loader, nine_language_test_loader = split_data(nine_language_dataset)\n",
    "four_language_train_loader, four_language_valid_loader, four_language_test_loader = split_data(four_language_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82bbaa2",
   "metadata": {},
   "source": [
    "### Definition of the Trainer class (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, valid_loader, test_loader, num_classes=2, num_epochs=10, patience=3, save_dir='checkpoints'):\n",
    "        self.device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.num_epochs = num_epochs\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08)\n",
    "        self.optimizer.zero_grad()\n",
    "        self.accuracy_metric = Accuracy(task=\"multiclass\", num_classes=num_classes).to(self.device)\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model_state = None\n",
    "        self.best_epoch = 0\n",
    "        self.l2_lambda = 0.001\n",
    "        self._initialize_requires_grad()\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "        model_name = model.__class__.__name__\n",
    "        self.model_save_dir = os.path.join(save_dir, model_name)\n",
    "        os.makedirs(self.model_save_dir, exist_ok=True)\n",
    "\n",
    "        self.patience = patience\n",
    "        self.early_stopping_counter = 0\n",
    "        self._save_config() #save the configs of the model in config.json\n",
    "\n",
    "    def _initialize_requires_grad(self):\n",
    "        # Make all parameters trainable\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "    def train(self, start_epoch=0):\n",
    "        for epoch in range(start_epoch, start_epoch + self.num_epochs):\n",
    "            self.model.train()\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            epoch_accuracy = 0.0\n",
    "\n",
    "            for inputs, targets in self.train_loader:\n",
    "                inputs, targets = inputs.float().to(self.device), targets.to(self.device)\n",
    "                pred = self.model(inputs)\n",
    "                print(\"train:\", pred, targets)\n",
    "                loss = self.criterion(pred, targets)\n",
    "                \n",
    "                # L2 Regularization\n",
    "                l2_norm = sum(p.pow(2).sum() for p in self.model.parameters())\n",
    "                loss += self.l2_lambda * l2_norm\n",
    "                \n",
    "                self.accuracy_metric.update(pred, targets)\n",
    "                accuracy = self.accuracy_metric.compute()\n",
    "                self.accuracy_metric.reset()\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_accuracy += accuracy.item()\n",
    "            \n",
    "            avg_loss = epoch_loss / len(self.train_loader)\n",
    "            avg_accuracy = epoch_accuracy / len(self.train_loader)\n",
    "\n",
    "            val_loss, val_accuracy = self.validate()\n",
    "            self.train_losses.append(float(avg_loss))\n",
    "            self.train_accuracies.append(float(avg_accuracy))\n",
    "            self.val_losses.append(float(val_loss))\n",
    "            self.val_accuracies.append(float(val_accuracy))\n",
    "\n",
    "            # Save current model\n",
    "            torch.save(self.model.state_dict(), os.path.join(self.model_save_dir, f'model_epoch_{epoch+1}.pt'))\n",
    "\n",
    "            # Best model logic\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_model_state = self.model.state_dict()\n",
    "                self.best_epoch = epoch + 1\n",
    "                self.early_stopping_counter = 0\n",
    "                self._save_best_model()\n",
    "                torch.save(self.best_model_state, os.path.join(self.model_save_dir, 'best_model.pt'))\n",
    "            else:\n",
    "                self.early_stopping_counter += 1\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{self.num_epochs}], Train Loss: {avg_loss:.4f}, Train Acc: {avg_accuracy:.4f}, '\n",
    "                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Patience Counter: {self.early_stopping_counter}')\n",
    "\n",
    "            # Check early stopping\n",
    "            if self.early_stopping_counter >= self.patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        self._load_best_model()\n",
    "        self.test()\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.valid_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                pred = self.model(inputs)\n",
    "                loss = self.criterion(pred, targets).item()\n",
    "                self.accuracy_metric.update(pred, targets)\n",
    "                accuracy = self.accuracy_metric.compute()\n",
    "                self.accuracy_metric.reset()\n",
    "\n",
    "                total_loss += loss\n",
    "                total_accuracy += accuracy.item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.valid_loader)\n",
    "        avg_accuracy = total_accuracy / len(self.valid_loader)\n",
    "        return avg_loss, avg_accuracy\n",
    "      \n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.test_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                pred = self.model(inputs)\n",
    "                loss = self.criterion(pred, targets).item()\n",
    "                self.accuracy_metric.update(pred, targets)\n",
    "                accuracy = self.accuracy_metric.compute()\n",
    "                self.accuracy_metric.reset()\n",
    "\n",
    "                total_loss += loss\n",
    "                total_accuracy += accuracy.item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.test_loader)\n",
    "        avg_accuracy = total_accuracy / len(self.test_loader)\n",
    "        print(f'Final Test Loss: {avg_loss:.4f}, Final Test Accuracy: {avg_accuracy:.4f}')\n",
    "\n",
    "    def _save_best_model(self):\n",
    "        with open(os.path.join(self.model_save_dir, \"best_model.pkl\"), \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"model_state\": self.best_model_state,\n",
    "                \"epoch\": self.best_epoch,\n",
    "                \"val_loss\": self.best_val_loss\n",
    "            }, f)\n",
    "\n",
    "    def _load_best_model(self):\n",
    "        with open(os.path.join(self.model_save_dir, \"best_model.pkl\"), \"rb\") as f:\n",
    "            saved_data = pickle.load(f)\n",
    "            self.model.load_state_dict(saved_data[\"model_state\"])\n",
    "            print(f\"Best Model Achieved at Epoch: {saved_data['epoch']} with Validation Loss: {saved_data['val_loss']:.4f}\")\n",
    "\n",
    "    def _save_config(self):\n",
    "        config = {\n",
    "            \"model_name\": self.model.__class__.__name__,\n",
    "            \"num_epochs\": self.num_epochs,\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"lr\": self.optimizer.defaults[\"lr\"],\n",
    "            \"betas\": self.optimizer.defaults[\"betas\"],\n",
    "            \"eps\": self.optimizer.defaults[\"eps\"],\n",
    "            \"loss_function\": \"CrossEntropyLoss\",\n",
    "            \"l2_lambda\": self.l2_lambda,\n",
    "            \"num_classes\": self.accuracy_metric.num_classes,\n",
    "            \"device\": str(self.device),\n",
    "            \"patience\": self.patience,\n",
    "        }\n",
    "        config_path = os.path.join(self.model_save_dir, \"config.json\")\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(config, f, indent=4)        \n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        plt.plot(self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss vs. Epoch')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracies(self):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(self.train_accuracies, label='Train Accuracy')\n",
    "        plt.plot(self.val_accuracies, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy vs. Epoch')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b2b95",
   "metadata": {},
   "source": [
    "## Trainer code for 9 languages. To see our final results, head to the section containing trainer code for 4 languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf89fa",
   "metadata": {},
   "source": [
    "### Training of ResNet34BiLSTMAttention (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet34BiLSTMAttention(classes=9)\n",
    "trainer = Trainer(model, nine_language_train_loader, nine_language_valid_loader, nine_language_test_loader, num_classes = 9, num_epochs=50, patience=10)\n",
    "trainer.train()\n",
    "trainer.plot_losses()\n",
    "trainer.plot_accuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b74ea",
   "metadata": {},
   "source": [
    "### Training of ResNet50BiLSTMAttention (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a695b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50BiLSTMAttention(classes=9)\n",
    "trainer = Trainer(model, nine_language_train_loader, nine_language_valid_loader, nine_language_test_loader, num_classes = 9, num_epochs=50, patience=10)\n",
    "trainer.train()\n",
    "trainer.plot_losses()\n",
    "trainer.plot_accuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e0a49",
   "metadata": {},
   "source": [
    "### Training of Resnet50 (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60dc622",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(classes=9)\n",
    "trainer = Trainer(model, nine_language_train_loader, nine_language_valid_loader, nine_language_test_loader, num_classes = 9, num_epochs=50, patience=10)\n",
    "trainer.train()\n",
    "trainer.plot_losses()\n",
    "trainer.plot_accuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f9da5",
   "metadata": {},
   "source": [
    "## Trainer code for 4 languages. This is our final results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc51832",
   "metadata": {},
   "source": [
    "### Training of ResNet34BiLSTMAttention (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdbf58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet34BiLSTMAttention(classes=4)\n",
    "trainer = Trainer(model, four_language_train_loader, four_language_valid_loader, four_language_test_loader, num_classes = 4, num_epochs=50, patience=10)\n",
    "trainer.train()\n",
    "trainer.plot_losses()\n",
    "trainer.plot_accuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b2cde8",
   "metadata": {},
   "source": [
    "### Training of ResNet50BiLSTMAttention (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91481fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50BiLSTMAttention(classes=4)\n",
    "trainer = Trainer(model, four_language_train_loader, four_language_valid_loader, four_language_test_loader, num_classes = 4, num_epochs=50, patience=10)\n",
    "trainer.train()\n",
    "trainer.plot_losses()\n",
    "trainer.plot_accuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966d063",
   "metadata": {},
   "source": [
    "### Training of ResNet50 (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cde6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(classes=4)\n",
    "trainer = Trainer(model, four_language_train_loader, four_language_valid_loader, four_language_test_loader, num_classes = 4, num_epochs=50, patience=10)\n",
    "trainer.train()\n",
    "trainer.plot_losses()\n",
    "trainer.plot_accuracies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
