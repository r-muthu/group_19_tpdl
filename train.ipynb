{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0772201",
   "metadata": {},
   "source": [
    "### This notebook contains the code to train the models, and load and test them. Run all cells sequentially. No modifications are needed unless stated\n",
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eddb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib pandas torch torchmetrics scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdecf8f6",
   "metadata": {},
   "source": [
    "### Import all libraries and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0de771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.classification import Accuracy\n",
    "from models import ResNet50, ResNet50BiLSTMAttention, ResNet34BiLSTMAttention\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "#Implemented seeding \n",
    "def seed_functions(seed):\n",
    "\t\"\"\"Seeds functions from numpy and torch.\"\"\"\n",
    "\tnp.random.seed(seed)\n",
    "\trandom.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed)\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "SEED = 37\n",
    "seed_functions(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13f548",
   "metadata": {},
   "source": [
    "### This is cell contains a custom class definition, helper function for instantiation to create a dataset of npy files (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43b94435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom class defined to store dataset\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, npy_file_paths, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            npy_file_paths (list of str): List of file paths for .npy files containing the sequences.\n",
    "            labels (list): List of labels corresponding to each sequence.\n",
    "        \"\"\"\n",
    "        # Load the sequences and labels\n",
    "        self.data = [torch.tensor(np.load(file_path)) for file_path in npy_file_paths]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)  # Convert the labels to a tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        # Dataset contains as many samples as the number of npy files\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the sequence data and its corresponding label\n",
    "        return self.data[idx], self.labels[idx].long()\n",
    "\n",
    "# Helper function to create dataset\n",
    "def create_dataset(path_to_dataset):\n",
    "    # Storing of dataset into class\n",
    "    npy_file_paths = []\n",
    "    labels = []\n",
    "\n",
    "    languages = sorted([d for d in os.listdir(path_to_dataset) if d != '.ipynb_checkpoints' and os.path.isdir(os.path.join(path_to_dataset, d))])\n",
    "    print(languages)\n",
    "    num_languages = len(languages)\n",
    "\n",
    "    for i, lang_dir in enumerate(languages):\n",
    "        lang_path = os.path.join(path_to_dataset, lang_dir)\n",
    "        if not os.path.isdir(lang_path):\n",
    "            continue  # Skip non-directory files\n",
    "\n",
    "        # List all .npy files\n",
    "        file_names = os.listdir(lang_path)\n",
    "        full_paths = [os.path.join(lang_path, f) for f in file_names]\n",
    "\n",
    "        # Extend lists\n",
    "        npy_file_paths.extend(full_paths)\n",
    "        labels.extend([i] * len(full_paths))\n",
    "\n",
    "\n",
    "    dataset = SequenceDataset(npy_file_paths, labels)  # THIS IS THE FINAL DATASET\n",
    "    return dataset, num_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356b7ec4",
   "metadata": {},
   "source": [
    "### Replace 'None' with the path to the dataset (MODIFY HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95c29218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'chinese', 'english', 'hindi']\n"
     ]
    }
   ],
   "source": [
    "path_to_dataset = 'data1' #Enter path to dataset here\n",
    "dataset, num_languages = create_dataset(path_to_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090d3ecc",
   "metadata": {},
   "source": [
    "### Helper function to split dataset into train, valid and test dataloaders (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e57df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    # Split dataset into train, validation, and test sets\n",
    "    indices = np.arange(len(dataset))\n",
    "    np.random.seed(SEED)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_size = int(0.7 * len(indices))\n",
    "    valid_size = (len(indices) - train_size) // 2\n",
    "    test_size = len(indices) - train_size - valid_size\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    valid_indices = indices[train_size:train_size+valid_size]\n",
    "    test_indices = indices[train_size +valid_size:]\n",
    "\n",
    "    train_data = torch.utils.data.Subset(dataset, train_indices)\n",
    "    valid_data = torch.utils.data.Subset(dataset, valid_indices)\n",
    "    test_data = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "    # Define a seed worker for DataLoader\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = SEED + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    # Define generators for DataLoader\n",
    "    generator = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "    batch_size = 128\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True,\n",
    "                              generator=generator, worker_init_fn=seed_worker, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_data, batch_size=len(valid_data), shuffle=False,\n",
    "                              generator=generator, worker_init_fn=seed_worker)\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False,\n",
    "                             generator=generator, worker_init_fn=seed_worker)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "train_loader, valid_loader, test_loader = split_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a8ebf1",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82bbaa2",
   "metadata": {},
   "source": [
    "### Definition of the Trainer class (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, valid_loader, test_loader, num_classes=2, num_epochs=10, patience=3, save_dir='checkpoints'):\n",
    "        self.device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.num_epochs = num_epochs\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08)\n",
    "        self.optimizer.zero_grad()\n",
    "        self.accuracy_metric = Accuracy(task=\"multiclass\", num_classes=num_classes).to(self.device)\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model_state = None\n",
    "        self.best_epoch = 0\n",
    "        self.l2_lambda = 0.001\n",
    "        self._initialize_requires_grad()\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "        model_name = model.__class__.__name__\n",
    "        self.model_save_dir = os.path.join(save_dir, model_name)\n",
    "        os.makedirs(self.model_save_dir, exist_ok=True)\n",
    "\n",
    "        self.patience = patience\n",
    "        self.early_stopping_counter = 0\n",
    "        self._save_config() #save the configs of the model in config.json\n",
    "\n",
    "    def _initialize_requires_grad(self):\n",
    "        # Make all parameters trainable\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "    def train(self, start_epoch=0):\n",
    "        for epoch in range(start_epoch, start_epoch + self.num_epochs):\n",
    "            self.model.train()\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            epoch_accuracy = 0.0\n",
    "\n",
    "            for inputs, targets in self.train_loader:\n",
    "                inputs, targets = inputs.float().to(self.device), targets.to(self.device)\n",
    "                pred = self.model(inputs)\n",
    "                loss = self.criterion(pred, targets)\n",
    "                \n",
    "                # L2 Regularization\n",
    "                l2_norm = sum(p.pow(2).sum() for p in self.model.parameters())\n",
    "                loss += self.l2_lambda * l2_norm\n",
    "                \n",
    "                self.accuracy_metric.update(pred, targets)\n",
    "                accuracy = self.accuracy_metric.compute()\n",
    "                self.accuracy_metric.reset()\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_accuracy += accuracy.item()\n",
    "            \n",
    "            avg_loss = epoch_loss / len(self.train_loader)\n",
    "            avg_accuracy = epoch_accuracy / len(self.train_loader)\n",
    "\n",
    "            val_loss, val_accuracy = self.validate()\n",
    "            self.train_losses.append(float(avg_loss))\n",
    "            self.train_accuracies.append(float(avg_accuracy))\n",
    "            self.val_losses.append(float(val_loss))\n",
    "            self.val_accuracies.append(float(val_accuracy))\n",
    "\n",
    "            # Save current model\n",
    "            torch.save(self.model.state_dict(), os.path.join(self.model_save_dir, f'model_epoch_{epoch+1}.pt'))\n",
    "\n",
    "            # Best model logic\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_model_state = self.model.state_dict()\n",
    "                self.best_epoch = epoch + 1\n",
    "                self.early_stopping_counter = 0\n",
    "                self._save_best_model()\n",
    "                torch.save(self.best_model_state, os.path.join(self.model_save_dir, 'best_model.pt'))\n",
    "            else:\n",
    "                self.early_stopping_counter += 1\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{self.num_epochs}], Train Loss: {avg_loss:.4f}, Train Acc: {avg_accuracy:.4f}, '\n",
    "                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Patience Counter: {self.early_stopping_counter}')\n",
    "\n",
    "            # Check early stopping\n",
    "            if self.early_stopping_counter >= self.patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        self._load_best_model()\n",
    "        self.test()\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.valid_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                pred = self.model(inputs)\n",
    "                loss = self.criterion(pred, targets).item()\n",
    "                self.accuracy_metric.update(pred, targets)\n",
    "                accuracy = self.accuracy_metric.compute()\n",
    "                self.accuracy_metric.reset()\n",
    "\n",
    "                total_loss += loss\n",
    "                total_accuracy += accuracy.item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.valid_loader)\n",
    "        avg_accuracy = total_accuracy / len(self.valid_loader)\n",
    "        return avg_loss, avg_accuracy\n",
    "      \n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.test_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                pred = self.model(inputs)\n",
    "                loss = self.criterion(pred, targets).item()\n",
    "                self.accuracy_metric.update(pred, targets)\n",
    "                accuracy = self.accuracy_metric.compute()\n",
    "                self.accuracy_metric.reset()\n",
    "\n",
    "                total_loss += loss\n",
    "                total_accuracy += accuracy.item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.test_loader)\n",
    "        avg_accuracy = total_accuracy / len(self.test_loader)\n",
    "        print(f'Final Test Loss: {avg_loss:.4f}, Final Test Accuracy: {avg_accuracy:.4f}')\n",
    "\n",
    "    def _save_best_model(self):\n",
    "        with open(os.path.join(self.model_save_dir, \"best_model.pkl\"), \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"model_state\": self.best_model_state,\n",
    "                \"epoch\": self.best_epoch,\n",
    "                \"val_loss\": self.best_val_loss\n",
    "            }, f)\n",
    "\n",
    "    def _load_best_model(self):\n",
    "        with open(os.path.join(self.model_save_dir, \"best_model.pkl\"), \"rb\") as f:\n",
    "            saved_data = pickle.load(f)\n",
    "            self.model.load_state_dict(saved_data[\"model_state\"])\n",
    "            print(f\"Best Model Achieved at Epoch: {saved_data['epoch']} with Validation Loss: {saved_data['val_loss']:.4f}\")\n",
    "\n",
    "    def _save_config(self):\n",
    "        config = {\n",
    "            \"model_name\": self.model.__class__.__name__,\n",
    "            \"num_epochs\": self.num_epochs,\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"lr\": self.optimizer.defaults[\"lr\"],\n",
    "            \"betas\": self.optimizer.defaults[\"betas\"],\n",
    "            \"eps\": self.optimizer.defaults[\"eps\"],\n",
    "            \"loss_function\": \"CrossEntropyLoss\",\n",
    "            \"l2_lambda\": self.l2_lambda,\n",
    "            \"num_classes\": self.accuracy_metric.num_classes,\n",
    "            \"device\": str(self.device),\n",
    "            \"patience\": self.patience,\n",
    "        }\n",
    "        config_path = os.path.join(self.model_save_dir, \"config.json\")\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(config, f, indent=4)        \n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        plt.plot(self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss vs. Epoch')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracies(self):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(self.train_accuracies, label='Train Accuracy')\n",
    "        plt.plot(self.val_accuracies, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy vs. Epoch')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b2b95",
   "metadata": {},
   "source": [
    "## Trainer code for 9 languages. To see our final results, head to the section containing trainer code for 4 languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf89fa",
   "metadata": {},
   "source": [
    "### Training of ResNet34BiLSTMAttention (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muthu\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muthu\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: train Shape: torch.Size([128, 1, 128, 241])\n",
      "ResNet Layer conv1 Output Shape: torch.Size([128, 64, 64, 121])\n",
      "ResNet Layer bn1 Output Shape: torch.Size([128, 64, 64, 121])\n",
      "ResNet Layer relu Output Shape: torch.Size([128, 64, 64, 121])\n",
      "ResNet Layer maxpool Output Shape: torch.Size([128, 64, 32, 61])\n",
      "ResNet Layer layer1 Output Shape: torch.Size([128, 64, 32, 61])\n",
      "ResNet Layer layer2 Output Shape: torch.Size([128, 128, 16, 31])\n",
      "ResNet Layer layer3 Output Shape: torch.Size([128, 256, 8, 16])\n",
      "ResNet Layer layer4 Output Shape: torch.Size([128, 512, 4, 8])\n",
      "ResNet Layer avgpool Output Shape: torch.Size([128, 512, 8, 8])\n",
      "train: tensor([[ 0.0468,  0.1238, -0.0036,  0.0526],\n",
      "        [ 0.0331,  0.1282,  0.0684,  0.0881],\n",
      "        [-0.0314,  0.0298, -0.0126,  0.0210],\n",
      "        [ 0.0421, -0.0674, -0.0530,  0.0943],\n",
      "        [-0.0299, -0.0163, -0.0441,  0.0210],\n",
      "        [ 0.0374,  0.0300, -0.0180,  0.0573],\n",
      "        [ 0.0540,  0.0949,  0.0278,  0.0715],\n",
      "        [-0.0776,  0.0757,  0.0145,  0.0236],\n",
      "        [ 0.1498,  0.0894, -0.0246,  0.0425],\n",
      "        [ 0.0072,  0.0162,  0.0128,  0.0390],\n",
      "        [-0.0518,  0.0830, -0.1130, -0.0006],\n",
      "        [ 0.0758,  0.0121,  0.0283,  0.0817],\n",
      "        [ 0.0126,  0.0817, -0.0153,  0.1059],\n",
      "        [ 0.0438,  0.0772, -0.0435,  0.0717],\n",
      "        [ 0.0444,  0.0432, -0.1090,  0.0628],\n",
      "        [ 0.0091,  0.0839, -0.0501,  0.0723],\n",
      "        [ 0.0135,  0.0473,  0.0034,  0.0295],\n",
      "        [-0.0158, -0.0474, -0.0297,  0.0111],\n",
      "        [ 0.0237,  0.0756, -0.0128,  0.0743],\n",
      "        [ 0.0210,  0.0409,  0.0103, -0.0140],\n",
      "        [-0.0258, -0.0203, -0.0323,  0.0634],\n",
      "        [ 0.0203, -0.0183, -0.0477,  0.0155],\n",
      "        [-0.0790, -0.0401, -0.0037,  0.0717],\n",
      "        [ 0.0179,  0.0108,  0.0177,  0.2522],\n",
      "        [ 0.0574,  0.0694,  0.0079,  0.1500],\n",
      "        [ 0.0801, -0.0112,  0.0465,  0.0450],\n",
      "        [-0.0399,  0.0221, -0.0036,  0.1314],\n",
      "        [ 0.0394, -0.0056, -0.0320,  0.0987],\n",
      "        [ 0.0488,  0.0383, -0.0116,  0.1398],\n",
      "        [-0.0981, -0.0722, -0.0331,  0.0718],\n",
      "        [-0.1471,  0.0840,  0.0462,  0.0991],\n",
      "        [-0.0061,  0.0228,  0.0142,  0.0474],\n",
      "        [ 0.0101,  0.0441, -0.1143, -0.0492],\n",
      "        [ 0.0062,  0.0424, -0.0472,  0.0572],\n",
      "        [ 0.0368,  0.0374, -0.0109,  0.0815],\n",
      "        [ 0.0102,  0.0313, -0.0920,  0.0152],\n",
      "        [-0.0466,  0.0526,  0.0788,  0.1516],\n",
      "        [-0.0802,  0.0777,  0.0314,  0.1048],\n",
      "        [ 0.0162,  0.0752, -0.0486, -0.0215],\n",
      "        [ 0.0367,  0.0373, -0.0537,  0.0265],\n",
      "        [-0.0238,  0.0365,  0.0111,  0.1393],\n",
      "        [-0.0687, -0.0198, -0.0963,  0.1353],\n",
      "        [ 0.0338,  0.0641, -0.0092,  0.1111],\n",
      "        [-0.0629, -0.0039, -0.1515,  0.0185],\n",
      "        [ 0.0014,  0.0548, -0.0420, -0.0310],\n",
      "        [-0.0481, -0.0078, -0.0719,  0.0715],\n",
      "        [ 0.1079, -0.0445,  0.0052,  0.0623],\n",
      "        [-0.0872,  0.0339, -0.0424,  0.1333],\n",
      "        [-0.0541,  0.0684, -0.0136,  0.1492],\n",
      "        [ 0.0094, -0.0580, -0.0754,  0.1379],\n",
      "        [-0.0289, -0.0090,  0.0757,  0.1323],\n",
      "        [-0.0514,  0.0740,  0.0698,  0.0704],\n",
      "        [ 0.1184,  0.0432, -0.0413,  0.0449],\n",
      "        [ 0.1003,  0.0824,  0.0013,  0.1029],\n",
      "        [ 0.0260,  0.0148,  0.0164,  0.0147],\n",
      "        [ 0.0362, -0.0972, -0.0262,  0.0491],\n",
      "        [-0.0081, -0.0197,  0.0390,  0.0535],\n",
      "        [-0.0292,  0.0157,  0.0568,  0.0974],\n",
      "        [-0.0009,  0.0727,  0.0635,  0.1299],\n",
      "        [ 0.0077,  0.0404, -0.0913,  0.0022],\n",
      "        [-0.0123, -0.0318,  0.0023,  0.1527],\n",
      "        [ 0.0247,  0.0458, -0.0183,  0.0720],\n",
      "        [-0.0741, -0.0398, -0.0164,  0.0199],\n",
      "        [ 0.0516,  0.1061,  0.0097,  0.0479],\n",
      "        [ 0.0606,  0.0991,  0.0324,  0.0430],\n",
      "        [-0.0538, -0.0251, -0.0298,  0.1584],\n",
      "        [-0.0462,  0.0545, -0.0043,  0.0980],\n",
      "        [ 0.0632,  0.0259, -0.0103, -0.0041],\n",
      "        [-0.0173,  0.0933, -0.0341,  0.0671],\n",
      "        [ 0.0289,  0.1017,  0.0533,  0.0946],\n",
      "        [-0.0459,  0.0433, -0.0855,  0.0485],\n",
      "        [-0.0467,  0.0896,  0.0235,  0.2249],\n",
      "        [ 0.0320, -0.0126, -0.0545,  0.0436],\n",
      "        [-0.0750, -0.0343, -0.0414,  0.1254],\n",
      "        [-0.0806,  0.0876, -0.0744,  0.1266],\n",
      "        [-0.0472,  0.0162,  0.0056,  0.1373],\n",
      "        [ 0.0293, -0.0680, -0.0564,  0.0838],\n",
      "        [ 0.0147, -0.0110,  0.0053,  0.0604],\n",
      "        [ 0.0350,  0.0056,  0.0496,  0.0448],\n",
      "        [ 0.0847, -0.0064, -0.0672,  0.0934],\n",
      "        [-0.0615,  0.0581, -0.0727, -0.0342],\n",
      "        [-0.0386, -0.0295,  0.0186,  0.0520],\n",
      "        [-0.0143,  0.0446,  0.0687,  0.0459],\n",
      "        [ 0.0115,  0.0269,  0.0383,  0.0052],\n",
      "        [ 0.0490, -0.0353, -0.0492,  0.0905],\n",
      "        [ 0.0529,  0.0933, -0.0421,  0.0449],\n",
      "        [-0.0107,  0.0934,  0.0292,  0.0403],\n",
      "        [ 0.0294, -0.0279,  0.0605,  0.1526],\n",
      "        [-0.0771, -0.0109,  0.0129,  0.0435],\n",
      "        [-0.0479,  0.0140, -0.0657, -0.0157],\n",
      "        [ 0.0300, -0.0352,  0.0483,  0.0902],\n",
      "        [-0.0188, -0.0035, -0.0579,  0.1448],\n",
      "        [-0.0299,  0.0973, -0.0577,  0.0406],\n",
      "        [ 0.0503,  0.0929,  0.0322,  0.1512],\n",
      "        [ 0.0789, -0.0529,  0.0370,  0.0844],\n",
      "        [-0.1557, -0.0163, -0.0617,  0.0441],\n",
      "        [ 0.0533, -0.0171,  0.0898,  0.0307],\n",
      "        [ 0.0155,  0.0010, -0.0670,  0.0087],\n",
      "        [ 0.0883,  0.0182,  0.0647,  0.0835],\n",
      "        [-0.0372,  0.1166, -0.0312,  0.0535],\n",
      "        [ 0.0065,  0.0800,  0.0493,  0.0860],\n",
      "        [-0.0101,  0.0628, -0.0246,  0.1803],\n",
      "        [ 0.0059,  0.0370, -0.0261,  0.0363],\n",
      "        [ 0.0473,  0.1296,  0.0059,  0.1049],\n",
      "        [ 0.0831,  0.0037, -0.0936,  0.0731],\n",
      "        [-0.0306,  0.0009,  0.0633,  0.0870],\n",
      "        [ 0.0230,  0.0218,  0.0629,  0.1562],\n",
      "        [-0.0340,  0.0006,  0.0018,  0.1707],\n",
      "        [-0.0155,  0.0987, -0.0381,  0.2053],\n",
      "        [ 0.0826,  0.0620,  0.0026,  0.1582],\n",
      "        [ 0.0735,  0.0440, -0.0138,  0.0673],\n",
      "        [-0.1091,  0.0154,  0.0780,  0.1007],\n",
      "        [-0.0434,  0.0536, -0.0338,  0.1288],\n",
      "        [-0.1017,  0.1278, -0.0899,  0.0099],\n",
      "        [-0.0276,  0.0193, -0.0383,  0.0854],\n",
      "        [-0.0226, -0.0297,  0.0580,  0.0172],\n",
      "        [ 0.0124,  0.0240, -0.0210,  0.1215],\n",
      "        [ 0.0601, -0.0870,  0.0724,  0.0999],\n",
      "        [-0.0267,  0.0699, -0.0234,  0.0684],\n",
      "        [ 0.0540, -0.0476, -0.0128,  0.0535],\n",
      "        [ 0.0388, -0.0601, -0.0179,  0.0790],\n",
      "        [-0.0608,  0.0686,  0.0267,  0.0686],\n",
      "        [-0.0268,  0.0225, -0.0406,  0.0292],\n",
      "        [ 0.0057, -0.0458, -0.0636,  0.0209],\n",
      "        [-0.0227, -0.0691, -0.0044,  0.1043],\n",
      "        [-0.0563,  0.1090,  0.0541,  0.1245],\n",
      "        [-0.0543, -0.0346,  0.0007,  0.0247],\n",
      "        [ 0.0202, -0.0184, -0.0348,  0.1775]], grad_fn=<AddmmBackward0>) tensor([3, 2, 0, 2, 2, 1, 3, 2, 0, 2, 2, 1, 3, 1, 3, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
      "        2, 3, 2, 3, 0, 1, 3, 0, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 1, 3, 2, 1, 0, 1,\n",
      "        2, 1, 1, 1, 0, 3, 2, 1, 3, 1, 3, 2, 1, 3, 1, 2, 1, 1, 3, 0, 2, 0, 1, 3,\n",
      "        3, 1, 1, 3, 2, 1, 1, 3, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 3, 2, 1, 2,\n",
      "        1, 2, 1, 2, 0, 1, 1, 2, 3, 1, 3, 1, 2, 0, 0, 3, 2, 2, 1, 3, 1, 1, 3, 3,\n",
      "        1, 3, 1, 1, 3, 3, 1, 3])\n",
      "Mode: train Shape: torch.Size([128, 1, 128, 241])\n",
      "ResNet Layer conv1 Output Shape: torch.Size([128, 64, 64, 121])\n",
      "ResNet Layer bn1 Output Shape: torch.Size([128, 64, 64, 121])\n",
      "ResNet Layer relu Output Shape: torch.Size([128, 64, 64, 121])\n",
      "ResNet Layer maxpool Output Shape: torch.Size([128, 64, 32, 61])\n"
     ]
    }
   ],
   "source": [
    "model = ResNet34BiLSTMAttention(classes=num_languages)\n",
    "trainer = Trainer(model, train_loader, valid_loader, test_loader, num_classes = num_languages, num_epochs=50, patience=10)\n",
    "trainer.train()\n",
    "trainer.plot_losses()\n",
    "trainer.plot_accuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b74ea",
   "metadata": {},
   "source": [
    "### Training of ResNet50BiLSTMAttention (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a695b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50BiLSTMAttention(classes=num_languages)\n",
    "trainer = Trainer(model, train_loader, valid_loader, test_loader, num_classes = num_languages, num_epochs=50, patience=10)\n",
    "trainer.train()\n",
    "trainer.plot_losses()\n",
    "trainer.plot_accuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e0a49",
   "metadata": {},
   "source": [
    "### Training of Resnet50 (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60dc622",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(classes=num_languages)\n",
    "trainer = Trainer(model, train_loader, valid_loader, test_loader, num_classes = num_languages, num_epochs=50, patience=10)\n",
    "trainer.train()\n",
    "trainer.plot_losses()\n",
    "trainer.plot_accuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a3fab0",
   "metadata": {},
   "source": [
    "## Loading and testing a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6d90b",
   "metadata": {},
   "source": [
    "### Helper function to load and test a model (no changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c37aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model_and_test(model_dir, model, test_loader, num_classes):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load model checkpoint\n",
    "    with open(os.path.join(model_dir, \"best_model.pkl\"), \"rb\") as f:\n",
    "        saved_data = pickle.load(f)\n",
    "        model.load_state_dict(saved_data[\"model_state\"])\n",
    "        print(f\"Best Model Achieved at Epoch: {saved_data['epoch']} with Validation Loss: {saved_data['val_loss']:.4f}\")\n",
    "    \n",
    "    # Setup accuracy metric\n",
    "    accuracy_metric = Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            accuracy_metric.update(outputs, targets)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    test_accuracy = accuracy_metric.compute().item()\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ffa76",
   "metadata": {},
   "source": [
    "### Load and test ResNet34BiLSTMAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf252b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"checkpoints/ResNet34BiLSTMAttentionlr0001\"\n",
    "model = ResNet34BiLSTMAttention(classes=num_languages)\n",
    "load_best_model_and_test(model_dir, model, test_loader, num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082bc69",
   "metadata": {},
   "source": [
    "### Load and test ResNet50BiLSTMAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fcc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"checkpoints/ResNet50BiLSTMAttentionlr0001\"\n",
    "model = ResNet50BiLSTMAttention(classes=num_languages)\n",
    "load_best_model_and_test(model_dir, model, test_loader, num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f5c745",
   "metadata": {},
   "source": [
    "### Load and test ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"checkpoints/ResNet500001\"\n",
    "model = ResNet50(classes=num_languages)\n",
    "load_best_model_and_test(model_dir, model, test_loader, num_classes=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
