{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0de771",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.classification import Accuracy\n",
    "from model import ResNetLSTM as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6624bce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b94435",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, npy_file_paths, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            npy_file_paths (list of str): List of file paths for .npy files containing the sequences.\n",
    "            labels (list): List of labels corresponding to each sequence.\n",
    "        \"\"\"\n",
    "        # Load the sequences and labels\n",
    "        self.data = [torch.tensor(np.load(file_path)) for file_path in npy_file_paths]\n",
    "        self.labels = torch.tensor(labels)  # Convert the labels to a tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        # Dataset contains as many samples as the number of npy files\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the sequence data and its corresponding label\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Example usage\n",
    "# Add paths to your .npy files containing sequences, using for/while loop\n",
    "npy_file_paths = [] # ['arabic0.npy', 'arabic1.npy', 'chinese0.npy'] \n",
    "\n",
    "# Corresponding labels for the sequences - depending on language (for loop)\n",
    "labels = [0, 0, 1]\n",
    "# adjust fc layer in models.py based on total number of languages\n",
    "\n",
    "dataset = SequenceDataset(npy_file_paths, labels)\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57df38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    # Split dataset into train, validation, and test sets\n",
    "    train_data, temp_data = train_test_split(dataset, test_size=0.3, shuffle=True)\n",
    "    valid_data, test_data = train_test_split(temp_data, test_size=0.5, shuffle=True)\n",
    "\n",
    "    # Define batch size for training, full batch for validation and testing\n",
    "    batch_size = 128\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)  # Mini-batch for training\n",
    "    valid_loader = DataLoader(valid_data, batch_size=len(valid_data), shuffle=False)  # Full batch for validation\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)  # Full batch for testing\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236705c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = split_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1f243",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, valid_loader, test_loader, num_epochs=10, gradual_unfreezing=False, unfreeze_epochs=[]):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.num_epochs = num_epochs\n",
    "        self.gradual_unfreezing = gradual_unfreezing\n",
    "        self.unfreeze_epochs = (\n",
    "            [] if unfreeze_epochs is None or not gradual_unfreezing \n",
    "            else sorted(unfreeze_epochs, reverse=True)  # Unfreeze last layers first\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08)\n",
    "        self.optimizer.zero_grad()\n",
    "        self.accuracy_metric = Accuracy(task=\"multiclass\", num_classes=self.model.num_classes).to(self.device)\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model_state = None\n",
    "        self.best_epoch = 0\n",
    "        self.l2_lambda = 0.001\n",
    "        self.layer_groups = self._group_layers()\n",
    "        self._initialize_requires_grad()\n",
    "\n",
    "    def _group_layers(self):\n",
    "        layer_dict = OrderedDict()\n",
    "        for name, param in self.model.named_parameters():\n",
    "            layer_name = name.split(\".\")[0]  # Extract top-level layer name\n",
    "            if layer_name not in layer_dict:\n",
    "                layer_dict[layer_name] = []\n",
    "            layer_dict[layer_name].append(param)\n",
    "        return list(layer_dict.items())  # List of (layer_name, parameters) tuples\n",
    "\n",
    "    def _initialize_requires_grad(self):\n",
    "        if not self.gradual_unfreezing:  # If gradual_unfreezing is False, make all parameters trainable\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for layer_name, params in self.layer_groups:\n",
    "                for param in params:\n",
    "                    param.requires_grad = False\n",
    "            self._unfreeze_layer() #unfreeze fc layer\n",
    "\n",
    "    def _unfreeze_layer_by_epoch(self, epoch):\n",
    "        if epoch in self.unfreeze_epochs:\n",
    "            self._unfreeze_layer()\n",
    "    \n",
    "    def _unfreeze_layer(self):\n",
    "        if self.layer_groups:\n",
    "            layer_name, params = self.layer_groups.pop()  # Unfreeze last remaining layer\n",
    "            for param in params:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            self._unfreeze_layer_by_epoch(epoch)\n",
    "            for inputs, targets in self.train_loader:\n",
    "                inputs, targets = inputs.float().to(self.device), targets.to(self.device)\n",
    "                pred = self.model(inputs)\n",
    "                loss = self.criterion(pred, targets)\n",
    "                \n",
    "                # L2 Regularization\n",
    "                l2_norm = sum(p.pow(2).sum() for p in self.model.parameters())\n",
    "                loss += self.l2_lambda * l2_norm\n",
    "                \n",
    "                self.accuracy_metric.update(pred, targets)\n",
    "                accuracy = self.accuracy_metric.compute()\n",
    "                self.accuracy_metric.reset()\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            val_loss, val_accuracy = self.validate()\n",
    "            \n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_model_state = self.model.state_dict()\n",
    "                self.best_epoch = epoch + 1\n",
    "                self._save_best_model()\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{self.num_epochs}], Training Loss: {loss.item():.4f}, Training Accuracy: {accuracy.item():.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        self._load_best_model()\n",
    "        self.test()\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = next(iter(self.valid_loader))\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            pred = self.model(inputs)\n",
    "            loss = self.criterion(pred, targets).item()\n",
    "            self.accuracy_metric.update(pred, targets)\n",
    "            accuracy = self.accuracy_metric.compute()\n",
    "            self.accuracy_metric.reset()\n",
    "        return loss, accuracy\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = next(iter(self.test_loader))\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            pred = self.model(inputs)\n",
    "            loss = self.criterion(pred, targets).item()\n",
    "            self.accuracy_metric.update(pred, targets)\n",
    "            accuracy = self.accuracy_metric.compute()\n",
    "            self.accuracy_metric.reset()\n",
    "        print(f'Final Test Loss: {loss:.4f}, Final Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    def _save_best_model(self):\n",
    "        with open(\"best_model.pkl\", \"wb\") as f:\n",
    "            pickle.dump({\"model_state\": self.best_model_state, \"epoch\": self.best_epoch, \"val_loss\": self.best_val_loss}, f)\n",
    "\n",
    "    def _load_best_model(self):\n",
    "        with open(\"best_model.pkl\", \"rb\") as f:\n",
    "            saved_data = pickle.load(f)\n",
    "            self.model.load_state_dict(saved_data[\"model_state\"])\n",
    "            print(f\"Best Model Achieved at Epoch: {saved_data['epoch']} with Validation Loss: {saved_data['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1c2f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(ResNet50(num_classes=3), train_loader, valid_loader, test_loader)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
